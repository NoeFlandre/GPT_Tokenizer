{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unicode code points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unicode code points are assigning an integer to every character and script across different writing systems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"N\") #using the ord function in python we can display the unicode encoding of a character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot use the unicode code points standard as our vocabulary for a LLM. First of all because we would end up with a gigantic vocabulary size but also because unicode is not a stable representation of the characters as it keeps on being updated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTF-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One encoding standard for representing every character in the unicode standard is UTF-8 which uses 1 to 4 bytes per character. The first 128 character of UTF-8 are the same than ASCII making UTF-8 backward compatible with ASCII. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[72, 101, 108, 108, 111, 32, 87, 111, 114, 108, 100, 33]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\"Hello World!\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot keep this as our vocabulary because we only a certain context windown size for our transformer and keeping utf_! just like so would make us end up with very long sequences adn therefore end up with an attention becoming extremely expensive. That's why we will use the byte pair encoding algorithm to leverage compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Byte Pair Encoding Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Byte Pair Encoding Algorithm rely on the following steps. Given a sequence of tokens and a vocabulary set, we are looking for the pair of juxtaposed tokens in the sequence occuring the most often. Once identified, we are creating a new token which is the concatenation of these two juxtaposed token (therefore augmenting our vocabulary size) and replacing them with our new token in the sequence. Thus the sequence is reducing in length and the vocabulary size is increasing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are at the very beginning of time for the human race. It is not unreasonable that we grapple with problems. But there are tens of thousands of years in the future. Our responsibility is to do what we can, learn what we can, improve the solutions, and pass them on. ðŸ”­\n",
      "Length of text : 269\n",
      "-------------------\n",
      "[87, 101, 32, 97, 114, 101, 32, 97, 116, 32, 116, 104, 101, 32, 118, 101, 114, 121, 32, 98, 101, 103, 105, 110, 110, 105, 110, 103, 32, 111, 102, 32, 116, 105, 109, 101, 32, 102, 111, 114, 32, 116, 104, 101, 32, 104, 117, 109, 97, 110, 32, 114, 97, 99, 101, 46, 32, 73, 116, 32, 105, 115, 32, 110, 111, 116, 32, 117, 110, 114, 101, 97, 115, 111, 110, 97, 98, 108, 101, 32, 116, 104, 97, 116, 32, 119, 101, 32, 103, 114, 97, 112, 112, 108, 101, 32, 119, 105, 116, 104, 32, 112, 114, 111, 98, 108, 101, 109, 115, 46, 32, 66, 117, 116, 32, 116, 104, 101, 114, 101, 32, 97, 114, 101, 32, 116, 101, 110, 115, 32, 111, 102, 32, 116, 104, 111, 117, 115, 97, 110, 100, 115, 32, 111, 102, 32, 121, 101, 97, 114, 115, 32, 105, 110, 32, 116, 104, 101, 32, 102, 117, 116, 117, 114, 101, 46, 32, 79, 117, 114, 32, 114, 101, 115, 112, 111, 110, 115, 105, 98, 105, 108, 105, 116, 121, 32, 105, 115, 32, 116, 111, 32, 100, 111, 32, 119, 104, 97, 116, 32, 119, 101, 32, 99, 97, 110, 44, 32, 108, 101, 97, 114, 110, 32, 119, 104, 97, 116, 32, 119, 101, 32, 99, 97, 110, 44, 32, 105, 109, 112, 114, 111, 118, 101, 32, 116, 104, 101, 32, 115, 111, 108, 117, 116, 105, 111, 110, 115, 44, 32, 97, 110, 100, 32, 112, 97, 115, 115, 32, 116, 104, 101, 109, 32, 111, 110, 46, 32, 240, 159, 148, 173]\n",
      "Length of tokens : 272\n"
     ]
    }
   ],
   "source": [
    "text = \"We are at the very beginning of time for the human race. It is not unreasonable that we grapple with problems. But there are tens of thousands of years in the future. Our responsibility is to do what we can, learn what we can, improve the solutions, and pass them on. ðŸ”­\"\n",
    "tokens = list(text.encode(\"utf-8\"))\n",
    "\n",
    "print(text)\n",
    "print(\"Length of text :\", len(text))\n",
    "print(\"-------------------\")\n",
    "\n",
    "print(tokens)\n",
    "print(\"Length of tokens :\", len(tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the length of the raw text and the length of the tokens are different because some characters are encoded using more than 1 byte. ASCII character are only taking 1 byte but others like emojis take more. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function which for a given sequence of tokens is outputing the stats of each pair of tokens, i.e the number of occurrence for each pair\n",
    "def find_pair_stats(tokens):\n",
    "    counts = {}\n",
    "    for pair in zip(tokens, tokens[1:]):\n",
    "        if pair in counts:\n",
    "            counts[pair]+=1\n",
    "        else :\n",
    "            counts[pair]=1\n",
    "    reversed_counts = {}\n",
    "    for pair, count in counts.items():\n",
    "        if count in reversed_counts:\n",
    "            reversed_counts[count].append(pair)\n",
    "        else : \n",
    "            reversed_counts[count] = [pair]\n",
    "    return reversed_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [(87, 101),\n",
       "  (32, 118),\n",
       "  (114, 121),\n",
       "  (32, 98),\n",
       "  (98, 101),\n",
       "  (101, 103),\n",
       "  (103, 105),\n",
       "  (110, 110),\n",
       "  (110, 105),\n",
       "  (110, 103),\n",
       "  (103, 32),\n",
       "  (109, 101),\n",
       "  (102, 111),\n",
       "  (111, 114),\n",
       "  (32, 104),\n",
       "  (104, 117),\n",
       "  (117, 109),\n",
       "  (109, 97),\n",
       "  (97, 99),\n",
       "  (99, 101),\n",
       "  (32, 73),\n",
       "  (73, 116),\n",
       "  (32, 110),\n",
       "  (110, 111),\n",
       "  (111, 116),\n",
       "  (32, 117),\n",
       "  (117, 110),\n",
       "  (110, 114),\n",
       "  (110, 97),\n",
       "  (97, 98),\n",
       "  (32, 103),\n",
       "  (103, 114),\n",
       "  (97, 112),\n",
       "  (112, 112),\n",
       "  (112, 108),\n",
       "  (119, 105),\n",
       "  (104, 32),\n",
       "  (111, 98),\n",
       "  (109, 115),\n",
       "  (115, 46),\n",
       "  (32, 66),\n",
       "  (66, 117),\n",
       "  (116, 101),\n",
       "  (101, 110),\n",
       "  (104, 111),\n",
       "  (111, 117),\n",
       "  (117, 115),\n",
       "  (115, 97),\n",
       "  (100, 115),\n",
       "  (32, 121),\n",
       "  (121, 101),\n",
       "  (114, 115),\n",
       "  (102, 117),\n",
       "  (116, 117),\n",
       "  (32, 79),\n",
       "  (79, 117),\n",
       "  (101, 115),\n",
       "  (115, 112),\n",
       "  (112, 111),\n",
       "  (115, 105),\n",
       "  (105, 98),\n",
       "  (98, 105),\n",
       "  (105, 108),\n",
       "  (108, 105),\n",
       "  (116, 121),\n",
       "  (116, 111),\n",
       "  (32, 100),\n",
       "  (100, 111),\n",
       "  (32, 108),\n",
       "  (114, 110),\n",
       "  (109, 112),\n",
       "  (111, 118),\n",
       "  (32, 115),\n",
       "  (111, 108),\n",
       "  (108, 117),\n",
       "  (105, 111),\n",
       "  (115, 44),\n",
       "  (100, 32),\n",
       "  (112, 97),\n",
       "  (115, 115),\n",
       "  (109, 32),\n",
       "  (110, 46),\n",
       "  (32, 240),\n",
       "  (240, 159),\n",
       "  (159, 148),\n",
       "  (148, 173)],\n",
       " 15: [(101, 32)],\n",
       " 4: [(32, 97),\n",
       "  (97, 114),\n",
       "  (97, 116),\n",
       "  (32, 111),\n",
       "  (46, 32),\n",
       "  (32, 105),\n",
       "  (111, 110),\n",
       "  (108, 101)],\n",
       " 6: [(114, 101), (104, 101), (115, 32), (32, 119)],\n",
       " 7: [(116, 32)],\n",
       " 11: [(32, 116)],\n",
       " 9: [(116, 104)],\n",
       " 2: [(118, 101),\n",
       "  (101, 114),\n",
       "  (121, 32),\n",
       "  (116, 105),\n",
       "  (105, 109),\n",
       "  (32, 102),\n",
       "  (114, 32),\n",
       "  (32, 114),\n",
       "  (114, 97),\n",
       "  (101, 46),\n",
       "  (105, 115),\n",
       "  (97, 115),\n",
       "  (115, 111),\n",
       "  (98, 108),\n",
       "  (105, 116),\n",
       "  (32, 112),\n",
       "  (112, 114),\n",
       "  (114, 111),\n",
       "  (101, 109),\n",
       "  (110, 100),\n",
       "  (117, 114),\n",
       "  (111, 32),\n",
       "  (119, 104),\n",
       "  (32, 99),\n",
       "  (99, 97),\n",
       "  (110, 44)],\n",
       " 3: [(105, 110),\n",
       "  (111, 102),\n",
       "  (102, 32),\n",
       "  (110, 32),\n",
       "  (101, 97),\n",
       "  (104, 97),\n",
       "  (119, 101),\n",
       "  (117, 116),\n",
       "  (110, 115),\n",
       "  (44, 32)],\n",
       " 5: [(97, 110)]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_pair_stats(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_common_pair(dictionary):\n",
    "    max = 0\n",
    "    for count, pair in dictionary.items():\n",
    "        if count > max :\n",
    "            max = count\n",
    "    return dictionary[max], ''.join(chr(i) for i in dictionary[max][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(101, 32)], 'e ')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = find_pair_stats(tokens)\n",
    "find_most_common_pair(dictionary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
